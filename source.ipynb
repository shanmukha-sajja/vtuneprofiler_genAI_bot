{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "093cf079-7ce2-4469-86a5-f2d978466e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/anaconda3/envs/v_test/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import chromadb\n",
    "import PyPDF2\n",
    "import time\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders.pdf import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering,AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "# import accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dff0c88-3097-480e-8fe0-3274fa44697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text(file_path):\n",
    "    pdf_file = open(file_path, 'rb')\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "    text = \"\"\n",
    "    for page_num in range( len(pdf_reader.pages)):\n",
    "        text += pdf_reader.pages[page_num].extract_text()\n",
    "    pdf_file.close()\n",
    "    return text\n",
    "\n",
    "# Initialize text splitter and embeddings\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=20)\n",
    "# Convert PDF to text\n",
    "text = pdf_to_text(os.path.join('./', './2024.pdf'))\n",
    "docs = [Document(page_content=x) for x in text_splitter.split_text(text)] #very important converting str to documents\n",
    "texts = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89924201-de51-44f5-9eef-4ac504e1e1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed and store the texts\n",
    "# Supplying a persist_directory will store the embeddings on disk\n",
    "\n",
    "persist_directory = 'db'\n",
    "## here we are using OpenAI embeddings but in future we will swap out to local embeddings\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=texts, \n",
    "                                 embedding=embedding,\n",
    "                                 persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690ddb53-dc63-4d1f-82cb-009d25855f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# persiste the db to disk\n",
    "vectordb.persist()\n",
    "vectordb = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3140db4-e7ad-4b7a-a0da-d6ed1c3114f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can load the persisted database from disk, and use it as normal. \n",
    "vectordb = Chroma(persist_directory=persist_directory, \n",
    "                  embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456423c1-c05b-4b7d-bc37-fefd2377d778",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mistralai--Mistral-7B\n",
    "\"\"\"\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig\n",
    "# Create a tokenizer object by loading the pretrained \"Intel/dynamic_tinybert\" tokenizer.\n",
    "# \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# \"deepset/roberta-base-squad2\"\n",
    "# model_path=\"/home/administrator/logs_jai/jai/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/73068f3702d050a2fd5aa2ca1e612e5036429398\"\n",
    "model_path=\"/home/administrator/logs_jai/jai/v/models--CohereForAI--c4ai-command-r-v01/snapshots/9c33b0976099d0f406f0d007613676fe42b78e3b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path,cache_dir=model_path)\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(model_path, cache_dir=model_path)\n",
    "\n",
    "# Create a question-answering model object by loading the pretrained \"Intel/dynamic_tinybert\" model.\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(model_path,cache_dir=model_path)\n",
    "model = LlamaForCausalLM.from_pretrained(model_path, cache_dir=model_path)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab029f-ad75-4bd2-8de8-4e50f221ff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "# torch.backends.cudnn.benchmark=True\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:16 \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe11975-b036-4e05-b46e-bc9aaaf3f766",
   "metadata": {},
   "outputs": [],
   "source": [
    "#zephyr-7b-->HuggingFaceH4/zephyr-7b-beta\n",
    "# NousResearch/Hermes-2-Pro-Mistral-7B\n",
    "#Xwin-LM/Xwin-LM-13B-V0.1\n",
    "# device_map=\"auto\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\", cache_dir=\"./\",use_fast=False,\n",
    "    trust_remote_code=True)\n",
    "# ,device_map=\"auto\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\", cache_dir=\"./\",torch_dtype=torch.bfloat16,trust_remote_code=True,load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870e5024-85e8-44e4-9975-0fdd3503d1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model name you want to use\n",
    "# model_name = \"./models--mistralai--Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "# Load the tokenizer associated with the specified model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Define a question-answering pipeline using the model and tokenizer\n",
    "# question_answerer = pipeline(\n",
    "#     \"text-generation\", \n",
    "#     model=model, \n",
    "#     tokenizer=tokenizer,'\n",
    "#     return_tensors='pt'\n",
    "# )\n",
    "\n",
    "generate_text = pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation', #question-ans,text2text,image2text \n",
    "    # we pass model parameters here too\n",
    "    temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    top_p=0.15,  # select from top tokens whose probability add up to 15%\n",
    "    top_k=0,  # select from top 0 tokens (because zero, relies on top_p)\n",
    "    max_new_tokens=2048,  # mex number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")\n",
    "\n",
    "# Create an instance of the HuggingFacePipeline, which wraps the question-answering pipeline\n",
    "# with additional model-specific arguments (temperature and max_length)\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=generate_text,\n",
    "    # model_kwargs={\"temperature\": 0.1, \"max_length\": 512},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53541f15-864d-4a15-a221-4cfecd799933",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380ca53c-4d56-4c5b-a7ce-bd8dfae4531c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=time.time()\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b8076e-d75e-437c-9407-f3811624910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"\"\"\n",
    "write a code for text summarization llm and that should run on NVIDA GPU and it should take only transformers/hugging face models as input\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b142bd3-d53b-47a3-84c5-cf534cfe844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "l=qa_chain(query)\n",
    "t2=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9dc01f-62e6-4e28-8944-309bdf3ec641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(l['query'])\n",
    "print(l['result'],t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d97f7f-6a74-4c6f-bbdd-072b6659496a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Cite sources\n",
    "# def process_llm_response(llm_response):\n",
    "#     print(llm_response['result'])\n",
    "#     print('\\n\\nSources:')\n",
    "#     for source in llm_response[\"source_documents\"]:\n",
    "#         print(source.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c95f880-63cf-48e0-92d7-78cc35577813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # full example\n",
    "# query = \"\"\"\n",
    "# give me some example on pointer to pointers in c with an example\n",
    "# \"\"\"\n",
    "# llm_response = qa_chain(query)\n",
    "# print(llm_response['result'])\n",
    "# # process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833f419e-2871-488f-97a7-cd8ed68653de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_test",
   "language": "python",
   "name": "v_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
